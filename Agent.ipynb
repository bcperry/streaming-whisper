{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d03ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level for semantic_kernel to DEBUG to see detailed request information\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "# Enable debug logging specifically for semantic kernel components\n",
    "logging.getLogger(\"semantic_kernel\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"semantic_kernel.connectors.ai.ollama\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"semantic_kernel.connectors.mcp\").setLevel(logging.DEBUG)\n",
    "\n",
    "# Enable HTTP-level logging to see actual POST request bodies\n",
    "logging.getLogger(\"httpx\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.DEBUG)\n",
    "\n",
    "# Also enable aiohttp if it's being used\n",
    "logging.getLogger(\"aiohttp\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc6c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request logging enabled!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "from functools import wraps\n",
    "\n",
    "# Store the original request method BEFORE we replace it\n",
    "_original_request = httpx.AsyncClient.request\n",
    "\n",
    "async def logged_request(self, method, url, **kwargs):\n",
    "    print(f\"\\n=== HTTP REQUEST ===\")\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"URL: {url}\")\n",
    "    \n",
    "    # Log headers\n",
    "    headers = kwargs.get('headers', {})\n",
    "    print(f\"Headers: {dict(headers) if headers else 'None'}\")\n",
    "    \n",
    "    # Log request body\n",
    "    content = kwargs.get('content')\n",
    "    json_data = kwargs.get('json')\n",
    "    data = kwargs.get('data')\n",
    "    \n",
    "    if content:\n",
    "        try:\n",
    "            # Try to parse as JSON for pretty printing\n",
    "            if isinstance(content, (str, bytes)):\n",
    "                content_str = content.decode() if isinstance(content, bytes) else content\n",
    "                parsed = json.loads(content_str)\n",
    "                print(f\"Request Body (JSON):\\n{json.dumps(parsed, indent=2)}\")\n",
    "            else:\n",
    "                print(f\"Request Body (Raw): {content}\")\n",
    "        except:\n",
    "            print(f\"Request Body (Raw): {content}\")\n",
    "    elif json_data:\n",
    "        print(f\"Request Body (JSON):\\n{json.dumps(json_data, indent=2)}\")\n",
    "    elif data:\n",
    "        print(f\"Request Body (Data): {data}\")\n",
    "    else:\n",
    "        print(\"Request Body: None\")\n",
    "    \n",
    "    print(f\"=== END REQUEST ===\\n\")\n",
    "    \n",
    "    # Call the ACTUAL original method, not the monkey-patched one\n",
    "    return await _original_request(self, method, url, **kwargs)\n",
    "\n",
    "# Apply the monkey patch\n",
    "httpx.AsyncClient.request = logged_request\n",
    "print(\"HTTP request logging enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252ef7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:14 - mcp.client.streamable_http:476 - DEBUG] Connecting to StreamableHTTP endpoint: http://localhost:8001/mcp\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:385 - DEBUG] Sending client message: root=JSONRPCRequest(method='initialize', params={'protocolVersion': '2025-06-18', 'capabilities': {'sampling': {}}, 'clientInfo': {'name': 'mcp', 'version': '0.1.0'}}, jsonrpc='2.0', id=0)\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBEFBBC0>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 21 Sep 2025 02:13:15 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'x-accel-buffering', b'no'), (b'Transfer-Encoding', b'chunked')])\n",
      "[2025-09-20 21:13:15 - httpx:1740 - INFO] HTTP Request: POST http://localhost:8001/mcp \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:134 - INFO] Received session ID: 884d426ecfc64579bf330d882d236595\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:163 - DEBUG] SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=0, result={'protocolVersion': '2025-06-18', 'capabilities': {'experimental': {}, 'prompts': {'listChanged': True}, 'resources': {'subscribe': False, 'listChanged': True}, 'tools': {'listChanged': True}}, 'serverInfo': {'name': 'demo_server', 'version': '1.13.1'}})\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:146 - INFO] Negotiated protocol version: 2025-06-18\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:13:15 - semantic_kernel.connectors.mcp:301 - DEBUG] Connected to MCP server: <mcp.client.session.ClientSession object at 0x0000026DDBEFADB0>\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:385 - DEBUG] Sending client message: root=JSONRPCNotification(method='notifications/initialized', params=None, jsonrpc='2.0')\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_body.failed exception=GeneratorExit()\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBF29730>\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBEFB0E0>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'GET']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'GET']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'GET']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 21 Sep 2025 02:13:15 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'x-accel-buffering', b'no'), (b'Transfer-Encoding', b'chunked')])\n",
      "[2025-09-20 21:13:15 - httpx:1740 - INFO] HTTP Request: GET http://localhost:8001/mcp \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:212 - DEBUG] GET SSE connection established\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'GET']>\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 202, b'Accepted', [(b'date', b'Sun, 21 Sep 2025 02:13:15 GMT'), (b'server', b'uvicorn'), (b'content-type', b'application/json'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'content-length', b'0')])\n",
      "[2025-09-20 21:13:15 - httpx:1740 - INFO] HTTP Request: POST http://localhost:8001/mcp \"HTTP/1.1 202 Accepted\"\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:266 - DEBUG] Received 202 Accepted\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:15 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:13:15 - mcp.client.streamable_http:385 - DEBUG] Sending client message: root=JSONRPCRequest(method='tools/list', params=None, jsonrpc='2.0', id=1)\n",
      "[2025-09-20 21:13:15 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBC0A900>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 21 Sep 2025 02:13:15 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'x-accel-buffering', b'no'), (b'Transfer-Encoding', b'chunked')])\n",
      "[2025-09-20 21:13:16 - httpx:1740 - INFO] HTTP Request: POST http://localhost:8001/mcp \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:163 - DEBUG] SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=1, result={'tools': [{'name': 'get_name', 'description': 'This tool returns a name.', 'inputSchema': {'properties': {'string': {'title': 'String', 'type': 'string'}}, 'required': ['string'], 'type': 'object'}, 'outputSchema': {'properties': {'result': {'title': 'Result', 'type': 'string'}}, 'required': ['result'], 'title': '_WrappedResult', 'type': 'object', 'x-fastmcp-wrap-result': True}, '_meta': {'_fastmcp': {'tags': []}}}]})\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:65 - DEBUG] Parsing decorator for function: get_name\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: kwargs\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: typing.Any\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:69 - DEBUG] annotations=[{'name': 'kwargs', 'is_required': True, 'type_': 'Any', 'type_object': typing.Any}]\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: return\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: list[semantic_kernel.contents.text_content.TextContent | semantic_kernel.contents.image_content.ImageContent | semantic_kernel.contents.binary_content.BinaryContent]\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: return\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: <class 'list'>\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: return\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: semantic_kernel.contents.text_content.TextContent | semantic_kernel.contents.image_content.ImageContent | semantic_kernel.contents.binary_content.BinaryContent\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: return\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: <class 'semantic_kernel.contents.text_content.TextContent'>\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: return\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: <class 'semantic_kernel.contents.image_content.ImageContent'>\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:137 - DEBUG] Parsing param: return\n",
      "[2025-09-20 21:13:16 - semantic_kernel.functions.kernel_function_decorator:138 - DEBUG] Parsing annotation: <class 'semantic_kernel.contents.binary_content.BinaryContent'>\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:385 - DEBUG] Sending client message: root=JSONRPCRequest(method='prompts/list', params=None, jsonrpc='2.0', id=2)\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.failed exception=GeneratorExit()\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBEFBB60>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 21 Sep 2025 02:13:15 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'x-accel-buffering', b'no'), (b'Transfer-Encoding', b'chunked')])\n",
      "[2025-09-20 21:13:16 - httpx:1740 - INFO] HTTP Request: POST http://localhost:8001/mcp \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:163 - DEBUG] SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=2, result={'prompts': []})\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:385 - DEBUG] Sending client message: root=JSONRPCRequest(method='logging/setLevel', params={'level': 'debug'}, jsonrpc='2.0', id=3)\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.failed exception=GeneratorExit()\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBF29610>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 21 Sep 2025 02:13:16 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'x-accel-buffering', b'no'), (b'Transfer-Encoding', b'chunked')])\n",
      "[2025-09-20 21:13:16 - httpx:1740 - INFO] HTTP Request: POST http://localhost:8001/mcp \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:163 - DEBUG] SSE message: root=JSONRPCError(jsonrpc='2.0', id=3, error=ErrorData(code=-32601, message='Method not found', data=None))\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:13:16 - semantic_kernel.connectors.mcp:313 - WARNING] Failed to set log level to 10\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:385 - DEBUG] Sending client message: root=JSONRPCRequest(method='tools/list', params=None, jsonrpc='2.0', id=4)\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.started host='localhost' port=8001 local_address=None timeout=30 socket_options=None\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.failed exception=GeneratorExit()\n",
      "[2025-09-20 21:13:16 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDBEFBF50>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 21 Sep 2025 02:13:16 GMT'), (b'server', b'uvicorn'), (b'cache-control', b'no-cache, no-transform'), (b'connection', b'keep-alive'), (b'content-type', b'text/event-stream'), (b'mcp-session-id', b'884d426ecfc64579bf330d882d236595'), (b'x-accel-buffering', b'no'), (b'Transfer-Encoding', b'chunked')])\n",
      "[2025-09-20 21:13:16 - httpx:1740 - INFO] HTTP Request: POST http://localhost:8001/mcp \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:16 - mcp.client.streamable_http:163 - DEBUG] SSE message: root=JSONRPCResponse(jsonrpc='2.0', id=4, result={'tools': [{'name': 'get_name', 'description': 'This tool returns a name.', 'inputSchema': {'properties': {'string': {'title': 'String', 'type': 'string'}}, 'required': ['string'], 'type': 'object'}, 'outputSchema': {'properties': {'result': {'title': 'Result', 'type': 'string'}}, 'required': ['result'], 'title': '_WrappedResult', 'type': 'object', 'x-fastmcp-wrap-result': True}, '_meta': {'_fastmcp': {'tags': []}}}]})\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:13:16 - root:11 - INFO] Successfully connected to MCP server: <test> with tools meta=None nextCursor=None tools=[Tool(name='get_name', title=None, description='This tool returns a name.', inputSchema={'properties': {'string': {'title': 'String', 'type': 'string'}}, 'required': ['string'], 'type': 'object'}, outputSchema={'properties': {'result': {'title': 'Result', 'type': 'string'}}, 'required': ['result'], 'title': '_WrappedResult', 'type': 'object', 'x-fastmcp-wrap-result': True}, annotations=None, meta={'_fastmcp': {'tags': []}})]\n",
      "[2025-09-20 21:13:16 - httpcore.http11:87 - DEBUG] receive_response_body.failed exception=GeneratorExit()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:15:45 - httpcore.http11:87 - DEBUG] receive_response_body.failed exception=ReadError(BrokenResourceError())\n",
      "[2025-09-20 21:15:45 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:15:45 - httpcore.http11:87 - DEBUG] response_closed.complete\n",
      "[2025-09-20 21:15:45 - mcp.client.streamable_http:218 - DEBUG] GET stream error (non-fatal): \n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "\n",
    "mcp_server = MCPStreamableHttpPlugin(\n",
    "                    name=\"test\",\n",
    "                    url=\"http://localhost:8001/mcp\",\n",
    "                )\n",
    "\n",
    "try:\n",
    "    await mcp_server.connect()\n",
    "    available_tools = await mcp_server.session.list_tools()\n",
    "    logging.info(f\"Successfully connected to MCP server: <{mcp_server.name}> with tools {available_tools}\")\n",
    "    # await mcp_server.close()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error connecting to {mcp_server.name} MCP server: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35de369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:33 - httpcore.connection:87 - DEBUG] connect_tcp.started host='ollama.home' port=80 local_address=None timeout=None socket_options=None\n",
      "[2025-09-20 21:13:33 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDF460740>\n",
      "[2025-09-20 21:13:33 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:33 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:33 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:33 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:33 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HTTP REQUEST ===\n",
      "Method: POST\n",
      "URL: /api/chat\n",
      "Headers: None\n",
      "Request Body (JSON):\n",
      "{\n",
      "  \"model\": \"phi4-mini:latest\",\n",
      "  \"stream\": false,\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"hi\"\n",
      "    }\n",
      "  ],\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"test-get_name\",\n",
      "        \"description\": \"This tool returns a name.\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"required\": [\n",
      "            \"string\"\n",
      "          ],\n",
      "          \"properties\": {\n",
      "            \"string\": {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "=== END REQUEST ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:34 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'openresty'), (b'Date', b'Sun, 21 Sep 2025 02:12:57 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'334'), (b'Connection', b'keep-alive'), (b'X-Served-By', b'ollama.home')])\n",
      "[2025-09-20 21:13:34 - httpx:1740 - INFO] HTTP Request: POST http://ollama.home/api/chat \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:34 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:34 - httpcore.http11:87 - DEBUG] receive_response_body.complete\n",
      "[2025-09-20 21:13:34 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:34 - httpcore.http11:87 - DEBUG] response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:45 - httpcore.connection:87 - DEBUG] close.started\n",
      "[2025-09-20 21:13:45 - httpcore.connection:87 - DEBUG] close.complete\n",
      "[2025-09-20 21:13:45 - httpcore.connection:87 - DEBUG] connect_tcp.started host='ollama.home' port=80 local_address=None timeout=None socket_options=None\n",
      "[2025-09-20 21:13:45 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDEFD2930>\n",
      "[2025-09-20 21:13:45 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:45 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:45 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:45 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:45 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HTTP REQUEST ===\n",
      "Method: POST\n",
      "URL: /api/chat\n",
      "Headers: None\n",
      "Request Body (JSON):\n",
      "{\n",
      "  \"model\": \"phi4-mini:latest\",\n",
      "  \"stream\": false,\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"hi\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Hello! How can I assist you today?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"get me a name\"\n",
      "    }\n",
      "  ],\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"test-get_name\",\n",
      "        \"description\": \"This tool returns a name.\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"required\": [\n",
      "            \"string\"\n",
      "          ],\n",
      "          \"properties\": {\n",
      "            \"string\": {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "=== END REQUEST ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:46 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'openresty'), (b'Date', b'Sun, 21 Sep 2025 02:13:09 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'796'), (b'Connection', b'keep-alive'), (b'X-Served-By', b'ollama.home')])\n",
      "[2025-09-20 21:13:46 - httpx:1740 - INFO] HTTP Request: POST http://ollama.home/api/chat \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:46 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:46 - httpcore.http11:87 - DEBUG] receive_response_body.complete\n",
      "[2025-09-20 21:13:46 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:46 - httpcore.http11:87 - DEBUG] response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > Sure, what type of names are we looking for?\n",
      "\n",
      "- Personal names (first/last)\n",
      "- Character names from books/movies/shows\n",
      "- Place/nationality/country names\n",
      "\n",
      "Please provide more details so that the tool's function is applied correctly. For example: You can use \"get me a name\" to get someone random, or you could specify like this:\n",
      "\n",
      "- Get me a first/last personal name.\n",
      "- Provide character from [book/movie/show].\n",
      "- Retrieve place/nationality/country names.\n",
      "\n",
      "Let me know your preference!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:54 - httpcore.connection:87 - DEBUG] close.started\n",
      "[2025-09-20 21:13:54 - httpcore.connection:87 - DEBUG] close.complete\n",
      "[2025-09-20 21:13:54 - httpcore.connection:87 - DEBUG] connect_tcp.started host='ollama.home' port=80 local_address=None timeout=None socket_options=None\n",
      "[2025-09-20 21:13:54 - httpcore.connection:87 - DEBUG] connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000026DDF462DB0>\n",
      "[2025-09-20 21:13:54 - httpcore.http11:87 - DEBUG] send_request_headers.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:54 - httpcore.http11:87 - DEBUG] send_request_headers.complete\n",
      "[2025-09-20 21:13:54 - httpcore.http11:87 - DEBUG] send_request_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:54 - httpcore.http11:87 - DEBUG] send_request_body.complete\n",
      "[2025-09-20 21:13:54 - httpcore.http11:87 - DEBUG] receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HTTP REQUEST ===\n",
      "Method: POST\n",
      "URL: /api/chat\n",
      "Headers: None\n",
      "Request Body (JSON):\n",
      "{\n",
      "  \"model\": \"phi4-mini:latest\",\n",
      "  \"stream\": false,\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"hi\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Hello! How can I assist you today?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"get me a name\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Sure, what type of names are we looking for?\\n\\n- Personal names (first/last)\\n- Character names from books/movies/shows\\n- Place/nationality/country names\\n\\nPlease provide more details so that the tool's function is applied correctly. For example: You can use \\\"get me a name\\\" to get someone random, or you could specify like this:\\n\\n- Get me a first/last personal name.\\n- Provide character from [book/movie/show].\\n- Retrieve place/nationality/country names.\\n\\nLet me know your preference!\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"use your tool\"\n",
      "    }\n",
      "  ],\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"test-get_name\",\n",
      "        \"description\": \"This tool returns a name.\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"required\": [\n",
      "            \"string\"\n",
      "          ],\n",
      "          \"properties\": {\n",
      "            \"string\": {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "=== END REQUEST ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-20 21:13:56 - httpcore.http11:87 - DEBUG] receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'openresty'), (b'Date', b'Sun, 21 Sep 2025 02:13:19 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'1335'), (b'Connection', b'keep-alive'), (b'X-Served-By', b'ollama.home')])\n",
      "[2025-09-20 21:13:56 - httpx:1740 - INFO] HTTP Request: POST http://ollama.home/api/chat \"HTTP/1.1 200 OK\"\n",
      "[2025-09-20 21:13:56 - httpcore.http11:87 - DEBUG] receive_response_body.started request=<Request [b'POST']>\n",
      "[2025-09-20 21:13:56 - httpcore.http11:87 - DEBUG] receive_response_body.complete\n",
      "[2025-09-20 21:13:56 - httpcore.http11:87 - DEBUG] response_closed.started\n",
      "[2025-09-20 21:13:56 - httpcore.http11:87 - DEBUG] response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > Certainly. I will use the function `test-get_name` to return an example of both types: \n",
      "\n",
      "1. A random last name (as a demonstration):\n",
      "```json\n",
      "{\n",
      "  \"type\": \"function\",\n",
      "  \"function\": {\n",
      "    \"name\": \"getName\",\n",
      "    \"description\": \"This tool returns one or more names.\",\n",
      "    \"parameters\": {\"type\": \"object\", \"required\": [\"num\"],\"properties\":{\"num\":{\"type\":\"integer\",\"description\": \"\"}}}\n",
      "  }\n",
      "}\n",
      "```\n",
      "And for a first name:\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John\"\n",
      "}\n",
      "```\n",
      "\n",
      "Let's execute the function with `test-get_name` to retrieve an example of both types:\n",
      "\n",
      "For last names (randomly generated):\n",
      "```json\n",
      "{ \n",
      "  \"type\":\"function\", \n",
      "  \"function\":{\n",
      "    \"name\":\"getName\",\n",
      "    \"description\":\"This tool returns one or more random names.\",\n",
      "    \"parameters\":{\"type\":\"object\",\"required\":[\"num\"],\"properties\":{\"num\":{\"type\":\"integer\",\"description\": \"\"}}}\n",
      "  }\n",
      "}\n",
      "\n",
      "```\n",
      "For first name, the result might look like this:\n",
      "```json\n",
      "{ \n",
      "  \"name\": \"John\"\n",
      "} \n",
      "\n",
      "```\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion\n",
    "chat_completion = OllamaChatCompletion(\n",
    "    # ai_model_id=\"gpt-oss:20b\",\n",
    "    ai_model_id=\"phi4-mini:latest\",\n",
    "    host=\"http://ollama.home\",\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# Set the logging level for  semantic_kernel.kernel to DEBUG.\n",
    "setup_logging()\n",
    "\n",
    "kernel.add_plugin(mcp_server)\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = OllamaChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "\n",
    "# Initiate a back-and-forth chat\n",
    "userInput = None\n",
    "while True:\n",
    "    # Collect user input\n",
    "    userInput = input(\"User > \")\n",
    "\n",
    "    # Terminate the loop if the user says \"exit\"\n",
    "    if userInput == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user input to the history\n",
    "    history.add_user_message(userInput)\n",
    "\n",
    "    # Get the response from the AI\n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + str(result))\n",
    "\n",
    "    # Add the message from the agent to the chat history\n",
    "    history.add_message(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5f499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
