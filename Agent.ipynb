{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d03ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level for semantic_kernel to DEBUG to see detailed request information\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "# Enable debug logging specifically for semantic kernel components\n",
    "logging.getLogger(\"semantic_kernel\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"semantic_kernel.connectors.ai.ollama\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"semantic_kernel.connectors.mcp\").setLevel(logging.DEBUG)\n",
    "\n",
    "# Enable HTTP-level logging to see actual POST request bodies\n",
    "logging.getLogger(\"httpx\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.DEBUG)\n",
    "\n",
    "# Also enable aiohttp if it's being used\n",
    "logging.getLogger(\"aiohttp\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import httpx\n",
    "# from functools import wraps\n",
    "\n",
    "# # Store the original request method BEFORE we replace it\n",
    "# _original_request = httpx.AsyncClient.request\n",
    "\n",
    "# async def logged_request(self, method, url, **kwargs):\n",
    "#     print(f\"\\n=== HTTP REQUEST ===\")\n",
    "#     print(f\"Method: {method}\")\n",
    "#     print(f\"URL: {url}\")\n",
    "    \n",
    "#     # Log headers\n",
    "#     headers = kwargs.get('headers', {})\n",
    "#     print(f\"Headers: {dict(headers) if headers else 'None'}\")\n",
    "    \n",
    "#     # Log request body\n",
    "#     content = kwargs.get('content')\n",
    "#     json_data = kwargs.get('json')\n",
    "#     data = kwargs.get('data')\n",
    "    \n",
    "#     if content:\n",
    "#         try:\n",
    "#             # Try to parse as JSON for pretty printing\n",
    "#             if isinstance(content, (str, bytes)):\n",
    "#                 content_str = content.decode() if isinstance(content, bytes) else content\n",
    "#                 parsed = json.loads(content_str)\n",
    "#                 print(f\"Request Body (JSON):\\n{json.dumps(parsed, indent=2)}\")\n",
    "#             else:\n",
    "#                 print(f\"Request Body (Raw): {content}\")\n",
    "#         except:\n",
    "#             print(f\"Request Body (Raw): {content}\")\n",
    "#     elif json_data:\n",
    "#         print(f\"Request Body (JSON):\\n{json.dumps(json_data, indent=2)}\")\n",
    "#     elif data:\n",
    "#         print(f\"Request Body (Data): {data}\")\n",
    "#     else:\n",
    "#         print(\"Request Body: None\")\n",
    "    \n",
    "#     print(f\"=== END REQUEST ===\\n\")\n",
    "    \n",
    "#     # Call the ACTUAL original method, not the monkey-patched one\n",
    "#     return await _original_request(self, method, url, **kwargs)\n",
    "\n",
    "# # Apply the monkey patch\n",
    "# httpx.AsyncClient.request = logged_request\n",
    "# print(\"HTTP request logging enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ef7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "\n",
    "mcp_server = MCPStreamableHttpPlugin(\n",
    "                    name=\"test\",\n",
    "                    url=\"http://localhost:8001/mcp\",\n",
    "                )\n",
    "\n",
    "try:\n",
    "    await mcp_server.connect()\n",
    "    available_tools = await mcp_server.session.list_tools()\n",
    "    logging.info(f\"Successfully connected to MCP server: <{mcp_server.name}> with tools {available_tools}\")\n",
    "    # await mcp_server.close()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error connecting to {mcp_server.name} MCP server: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35de369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings import OllamaChatPromptExecutionSettings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI chat completion\n",
    "chat_completion = OllamaChatCompletion(\n",
    "    ai_model_id=\"gpt-oss:20b\",\n",
    "    host=\"http://ollama.home\",\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# Set the logging level for  semantic_kernel.kernel to DEBUG.\n",
    "setup_logging()\n",
    "\n",
    "kernel.add_plugin(mcp_server)\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = OllamaChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "\n",
    "# Initiate a back-and-forth chat\n",
    "userInput = None\n",
    "while True:\n",
    "    # Collect user input\n",
    "    userInput = input(\"User > \")\n",
    "\n",
    "    # Terminate the loop if the user says \"exit\"\n",
    "    if userInput == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user input to the history\n",
    "    history.add_user_message(userInput)\n",
    "\n",
    "    # Get the response from the AI\n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + str(result))\n",
    "\n",
    "    # Add the message from the agent to the chat history\n",
    "    history.add_message(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5f499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
